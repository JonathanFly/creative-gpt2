{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2 - ban tokens (by index or letter).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madscience101/creative-gpt2/blob/master/snippets/GPT2_ban_tokens_(by_index_or_letter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFMcqsUS8ee2",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMKNeOkO8Kj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_imBBHVKJ-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_pretrained_bert import GPT2Tokenizer,GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vTMgYB7LKev",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XmAnLyKQ5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UZ9eFAvKphK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer, GPT2Model\n",
        "\n",
        "import logging\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "from tqdm import trange\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ00PuFngLq3",
        "colab_type": "text"
      },
      "source": [
        "# Init the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAX6LPuRLj6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "enc = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBsEn3gYTyjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.to(device)\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WllfZ0VIkhOM",
        "colab_type": "text"
      },
      "source": [
        "# Banning tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExURXSnulqOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_k_logits(logits, k):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFIHoDNEg4_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_masked(model, length,banned, context=None, temperature=1, top_k=50, device='cuda'):\n",
        "    assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    context = torch.tensor(context, device=device, dtype=torch.long).unsqueeze(0)\n",
        "    \n",
        "    prev = context\n",
        "    output = context\n",
        "    past = None\n",
        "    with torch.no_grad():\n",
        "        for i in range(length):\n",
        "            logits, past = model(prev, past=past)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            \n",
        "            logits = top_k_logits(logits, k=top_k)\n",
        "            logits = logits.index_fill_(-1, banned,-1e10)\n",
        "\n",
        "            log_probs = F.softmax(logits, dim=-1)\n",
        "            prev = torch.multinomial(log_probs, num_samples=1)    \n",
        "            output = torch.cat((output, prev), dim=1)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xy28JVlqla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMROuMLjnhPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_samples_masked(prompt,model,device,enc,banned_tokens=[50256],\n",
        "              nsamples=1,length=200,temperature=1, \n",
        "              top_k=40, seed=0):\n",
        "   \n",
        "    banned_idx = torch.LongTensor(banned_tokens).to(device)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    context_tokens = enc.encode(prompt)\n",
        "        \n",
        "    generated = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      for _ in range(nsamples):\n",
        "              out = get_sequence_masked(\n",
        "                  model=model, length=length,\n",
        "                  context=context_tokens,\n",
        "                  banned = banned_idx,\n",
        "                  temperature=temperature, top_k=top_k, device=device\n",
        "              )\n",
        "\n",
        "              out_trimmed = out[:, len(context_tokens):].tolist()[0]\n",
        "              generated += 1\n",
        "              text = enc.decode(out_trimmed)\n",
        "              print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              print(text)\n",
        "      print(\"=\" * 80)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHUws9b4qCHj",
        "colab_type": "text"
      },
      "source": [
        "## TEST - ban tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ETYsoubJffig",
        "colab": {}
      },
      "source": [
        "global_banned_tokens = [50256] #EndOfText\n",
        "local_banned_tokens = [198, 628] #\\n and \\n\\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hUDi9xavffii",
        "colab": {}
      },
      "source": [
        "banned_tokens = global_banned_tokens+local_banned_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJdZujJ_nhSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompt = 'In the beginning'\n",
        "\n",
        "generate_samples_masked(prompt, model, device, enc, nsamples=3, length=30, banned_tokens=banned_tokens, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxw8KWHvnhXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiwwvychgXzA",
        "colab_type": "text"
      },
      "source": [
        "# Ban tokens with a specific letter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsXAFcRqU0D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_TOKENS = len(enc.encoder)\n",
        "TOKENS = np.array([enc.decode([i]) for i in range(NUM_TOKENS)])\n",
        "len(TOKENS), NUM_TOKENS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjBVrhGZZnNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOKENS_LOWER = [t.lower() for t in TOKENS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHwH8NcihYZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_with_letter(letter):\n",
        "  letter_in_tokens = np.array([letter in t for t in TOKENS_LOWER])\n",
        "  return(np.where(letter_in_tokens==True)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCQnEph0gOZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CauXitbnhVen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "letter = 't'\n",
        "banned_by_letter = tokens_with_letter(letter)\n",
        "\n",
        "prompt = 'In the beginning,'\n",
        "generate_samples_masked(prompt, model, device, enc, nsamples=3, length=30, banned_tokens=banned_by_letter, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0kx3B_jslM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}